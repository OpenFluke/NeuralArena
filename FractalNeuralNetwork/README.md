# Report on Recursive “Neurons-with-Subnetworks” Architecture (Experiment 58)

## 1. Relation to Known Neural Network Architectures

**Nested (Recursive) Networks:** The described architecture—where each neuron in a hidden layer contains an entire sub-network (called a “Dimension”)—closely resembles _nested neural networks_. In recent literature, Shen _et al._ introduce the concept of adding a third dimension (“height”) to network design: a _NestNet_ of height _s_ is constructed recursively such that _each hidden neuron is itself activated by a sub-network (NestNet) of height ≤ s–1_ ([](https://proceedings.neurips.cc/paper_files/paper/2022/file/257be12f31dfa7cc158dda99822c6fd1-Paper-Conference.pdf#:~:text=a%20nested%20structure%2C%20and%20hence,Lipschitz%20continuous)). In other words, a height-2 NestNet would have neurons in one layer that each encapsulate a smaller neural network (height-1) as their activation function. This directly parallels the user’s design in Experiment 58. Such nested architectures are fundamentally compositional, building complex functions by recursively stacking smaller networks inside neurons. They have been referred to as **compositional** or **nested networks** in the literature, highlighting that the overall model is a composition of learned sub-network “modules.”

**Network-in-Network (NiN):** An earlier related idea is the _Network-in-Network_ architecture by Lin _et al._ ([](https://arxiv.org/pdf/1312.4400#:~:text=nonlinear%20function%20of%20the%20input,Both%20the)). NiN replaces the simple linear neuron (e.g. a convolution filter followed by nonlinearity) with a “micro-network” (typically a small multilayer perceptron) operating on local input patches. Essentially, each location in a feature map is computed by a tiny neural network rather than a single linear filter ([](https://arxiv.org/pdf/1312.4400#:~:text=nonlinear%20function%20of%20the%20input,Both%20the)). While NiN was applied within convolutional networks (using 1×1 conv layers as MLPs per pixel), it shares the spirit of _increasing model complexity by having networks inside the network_. Your “Dimension” sub-network per neuron is a generalization of this idea to fully-connected layers: each neuron's output is produced by a subnetwork, not just a weighted sum. This kind of design improves the function approximation power of each neuron; indeed, NiN was shown to enhance local feature modeling and yielded strong results on CIFAR-10/100 by using MLPs as neurons ([](https://arxiv.org/pdf/1312.4400#:~:text=nonlinear%20function%20of%20the%20input,Both%20the)).

**Fractal and Self-Similar Networks:** The recursive, hierarchical nature of your architecture also evokes **FractalNet** and other self-similar architectures. FractalNet (Larsson _et al._, 2017) is a convolutional network built by repeatedly applying an expansion rule to create a self-similar (fractal) topology ([[1605.07648] FractalNet: Ultra-Deep Neural Networks without Residuals](https://arxiv.org/abs/1605.07648#:~:text=based%20on%20self,Rather%2C%20the%20key%20may)). The resulting network contains multiple intertwined paths of different lengths (depths) but _no explicit skip/residual connections_. Instead, the fractal structure itself provides both shallow and deep subpaths. Notably, FractalNet achieved performance on par with state-of-the-art ResNets on CIFAR and ImageNet ([[1605.07648] FractalNet: Ultra-Deep Neural Networks without Residuals](https://arxiv.org/abs/1605.07648#:~:text=is%20transformed%20by%20a%20filter,performance)), demonstrating that a recursively constructed _self-similar_ network can be as powerful as residual networks. The key insight from FractalNet is that **the network can transition from effectively shallow to deep during training**, as shorter subpaths learn first and deeper subpaths gradually contribute ([[1605.07648] FractalNet: Ultra-Deep Neural Networks without Residuals](https://arxiv.org/abs/1605.07648#:~:text=standard%20residual%20networks%20on%20both,quick%20answer%2C%20while%20deeper%20subnetworks)). This was facilitated by a technique called _drop-path_, which randomly drops whole branches during training to ensure all fractal sub-networks learn to solve the task ([[1605.07648] FractalNet: Ultra-Deep Neural Networks without Residuals](https://arxiv.org/abs/1605.07648#:~:text=the%20success%20of%20extremely%20deep,provide%20a%20more%20accurate%20answer)). Your neuron-with-subnetwork design is similarly a multi-level hierarchy. It can be seen as a fractal structure unrolled in depth: each neuron’s subnetwork adds an inner depth, creating paths through the overall network that can become very deep (going through neurons that themselves have internal layers).

**Hypernetworks:** Although the term _hypernetwork_ was mentioned, your implementation is conceptually different from classical **Hypernetworks**. In a hypernetwork (Ha _et al._, 2016), one network generates the weights for another network ([[1701.06538] Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer](https://arxiv.org/abs/1701.06538#:~:text=improvements%20in%20model%20capacity%20with,On%20large%20language%20modeling%20and)). For example, a small network outputs the weight matrix of a larger network dynamically. In your case, the “Dimension” sub-network is not generating weights for the main network; it is directly part of the computational graph producing the neuron's output. Thus, a closer match is the nested/NiN/fractal family where networks are embedded within networks, rather than hypernetworks which are about weight generation. (However, it’s worth noting that both approaches increase parameterization and expressiveness — hypernetworks by indirect _weight_ modeling, and nested networks by direct _function_ composition.)

**Neural Module Networks:** Another related concept is **Neural Module Networks** (Andreas _et al._, 2016), especially in terms of compositional design. In neural module networks, a model is composed of multiple smaller neural modules assembled dynamically (often guided by an input’s structure, such as a question’s parse tree) ([[1511.02799] Neural Module Networks](https://arxiv.org/abs/1511.02799#:~:text=compositional%20linguistic%20structure%20of%20questions,a%20new%20dataset%20of%20complex)). For instance, different sub-networks might handle different question subparts, and they are composed to answer a complex query. This is a coarse-grained form of recursion: each _module_ (sub-network) handles a portion of the problem, and the overall network is built by wiring these modules together for each input. Your architecture, by contrast, is a fixed wiring (not input-dependent) but is _fine-grained_: every single neuron may be a learned module. Both approaches embrace the principle of **composition** – assembling complex behaviors from simpler learned components. Module networks have shown that compositional structures can achieve state-of-the-art results in tasks like visual question answering by leveraging reusable specialized sub-networks ([[1511.02799] Neural Module Networks](https://arxiv.org/abs/1511.02799#:~:text=compositional%20linguistic%20structure%20of%20questions,a%20new%20dataset%20of%20complex)). The difference is that module networks are typically designed manually or via program logic for a task, whereas your approach _evolves_ and learns the subnetwork structure for each neuron automatically.

In summary, the closest literature analog to your design is the idea of **nested or fractal networks**. The term “recursive neural network” is sometimes used for tree-structured networks (e.g. in natural language parse trees), but here it’s more about hierarchical composition in the architecture itself. Phrasing like _NestedNet_, _fractal architecture_, or _network-in-network_ is appropriate. Your “Dimension” sub-networks essentially act as learned activation functions of high complexity, akin to the learned sub-network activation (ϱ) in NestNet ([](https://proceedings.neurips.cc/paper_files/paper/2022/file/257be12f31dfa7cc158dda99822c6fd1-Paper-Conference.pdf#:~:text=problems%3A%20a%20synthetic%20classification%20problem,to%20R%20d%20if%20each)) ([](https://proceedings.neurips.cc/paper_files/paper/2022/file/257be12f31dfa7cc158dda99822c6fd1-Paper-Conference.pdf#:~:text=a%20nested%20structure%2C%20and%20hence,Lipschitz%20continuous)). This places your work among architectures exploring **depth beyond depth** – adding internal depth to neurons, rather than just stacking more layers in series.

## 2. Performance of Recursive/Fractal Architectures on Benchmark Tasks

**Synthetic Data (Pattern Learning):** Recursive and fractal-like architectures have shown promising results on synthetic tasks that require hierarchical function approximation. For example, Shen _et al._ evaluated NestNets on a synthetic binary classification problem based on Archimedean spirals ([](https://proceedings.neurips.cc/paper_files/paper/2022/file/257be12f31dfa7cc158dda99822c6fd1-Paper-Conference.pdf#:~:text=optimization%20algorithms%20are%20developed%20for,continuously%20extended%20to%20R%20d)) ([](https://proceedings.neurips.cc/paper_files/paper/2022/file/257be12f31dfa7cc158dda99822c6fd1-Paper-Conference.pdf#:~:text=3,Archimedean%20spirals%29%20as%20follows)) – a task that is not linearly separable and benefits from complex decision boundaries. In their experiments, a small fully-connected NestNet (with neurons having a one-hidden-layer subnetwork) significantly outperformed a standard MLP of the same size ([](https://proceedings.neurips.cc/paper_files/paper/2022/file/257be12f31dfa7cc158dda99822c6fd1-Paper-Conference.pdf#:~:text=Table%202%3A%20Test%20accuracy%20comparison,999984)). With four hidden layers of width 20, the standard network achieved ~73.8% accuracy, whereas the NestNet (each neuron with a 10-parameter subnetwork) reached ~87.4% ([](https://proceedings.neurips.cc/paper_files/paper/2022/file/257be12f31dfa7cc158dda99822c6fd1-Paper-Conference.pdf#:~:text=Table%202%3A%20Test%20accuracy%20comparison,999984)). As the width increased, the advantage grew: at width 35, the NestNet attained **99.6%** accuracy vs **81.6%** for the plain network, and at width 50 it essentially solved the task (~99.998% accuracy) vs ~86.6% for the plain network ([](https://proceedings.neurips.cc/paper_files/paper/2022/file/257be12f31dfa7cc158dda99822c6fd1-Paper-Conference.pdf#:~:text=standard%20network%2020%204%201362,999984)). This dramatic improvement on the spiral dataset demonstrates the **“super-approximation” power** of nested subnetworks in capturing complex curves ([](https://proceedings.neurips.cc/paper_files/paper/2022/file/257be12f31dfa7cc158dda99822c6fd1-Paper-Conference.pdf#:~:text=Table%202%3A%20Test%20accuracy%20comparison,999984)). Essentially, the fractal/nested architecture can represent much more intricate decision boundaries with the same number of top-level neurons, confirming theoretical claims that adding this recursive dimension boosts expressiveness ([](https://proceedings.neurips.cc/paper_files/paper/2022/file/257be12f31dfa7cc158dda99822c6fd1-Paper-Conference.pdf#:~:text=net%02work%20%28NestNet%29,while%20the%20optimal%20approximation)) ([](https://proceedings.neurips.cc/paper_files/paper/2022/file/257be12f31dfa7cc158dda99822c6fd1-Paper-Conference.pdf#:~:text=height%2C%20in%20addition%20to%20width,parameters%29.%20The)).

Another synthetic example is the **hierarchical XOR (parity) problem**, a classic benchmark for compositional learning. The _N_-bit parity function (XOR of N bits) is difficult for shallow networks because it requires learning a highly non-linear, combinatorial function. Researchers have experimented with nested structures to tackle this. Rahmat _et al._ proposed a “Gene-Regulated Nested Neural Network” (GRNNN) and validated it on an 8-bit parity classification task ([A Gene-Regulated Nested Neural Network](https://ccis2k.org/iajit/index.php/archive/volume-12-2015/november-2015-no-6/item/1429-a-gene-regulated-nested-neural-network#:~:text=machine%20development%20and%20knowledge%20discovery,and%20meets%20the%20required%20objectives)). Their nested architecture (trained with a genetic algorithm-inspired scheme) accurately learned the 8-bit XOR without excessive training time ([A Gene-Regulated Nested Neural Network](https://ccis2k.org/iajit/index.php/archive/volume-12-2015/november-2015-no-6/item/1429-a-gene-regulated-nested-neural-network#:~:text=machine%20development%20and%20knowledge%20discovery,and%20meets%20the%20required%20objectives)). This indicates that recursive subnetworks can capture the **hierarchical binary logic** (XOR of XORs, recursively) effectively, whereas a standard network might struggle or need many more neurons. The success on parity problems suggests that a fractal-like design is naturally suited for tasks that are _inherently compositional_, where a solution can be built by recursively combining simpler solutions (e.g., XOR of smaller bit groups).

**Sparse or Hierarchical Feature Detection:** Tasks where only a few features are relevant (sparse signal) or where features must be combined in a hierarchical way can also benefit from such architectures. For instance, a deeply nested network could first detect low-level patterns in its inner subnetworks and then have higher-level neurons combine those patterns. While we haven’t seen a direct one-to-one published experiment for “sparse feature detection” using neuron-level subnetworks, we can draw parallels to **Mixture-of-Experts (MoE)** models. An MoE can be viewed as having multiple sub-networks (experts) where only some are active for a given input ([[1701.06538] Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer](https://arxiv.org/abs/1701.06538#:~:text=improvements%20in%20model%20capacity%20with,On%20large%20language%20modeling%20and)). Sparsely-Gated MoEs (Shazeer _et al._, 2017) consist of _up to thousands of feed-forward sub-networks (experts)_, with a gating mechanism learning to activate only a few for each example ([[1701.06538] Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer](https://arxiv.org/abs/1701.06538#:~:text=improvements%20in%20model%20capacity%20with,On%20large%20language%20modeling%20and)). In large-scale language tasks, MoEs achieved **significantly better results than previous state-of-the-art models at a fraction of the computational cost**, by leveraging many specialized sub-networks ([[1701.06538] Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer](https://arxiv.org/abs/1701.06538#:~:text=Experts%20layer%20,art%20at%20lower%20computational%20cost)). This highlights that having numerous sub-network components can dramatically increase model capacity and handle varied patterns in data. Your architecture, in principle, has a “mixture” at each neuron (though without a gating function, it implicitly uses all components via the nested computation). It could be advantageous in scenarios where different neurons (with their subnetworks) specialize in detecting different patterns or combinations of inputs.

**Long-Range Dependencies:** When it comes to sequence tasks or problems with long-range dependencies (e.g., time-series with long-term patterns, or text with long contexts), naive deep networks often struggle due to vanishing gradients. Gating architectures like **LSTM recurrent networks** and self-attention in **Transformers** have been the go-to solutions, as they explicitly address long-range dependency learning. A recursive nested network without special gating is essentially a very deep computation unrolled (each subnetwork adds depth). This is analogous to a _deep recurrent network_ unfolded for a fixed number of steps. In fact, a StackExchange discussion noted that _nesting a network within itself is like a very short recurrent network,_ and if you keep nesting deeper, you encounter the **same issues as deep recurrence: vanishing or exploding gradients** ([machine learning - What is the potential issue of nested neural networks - Artificial Intelligence Stack Exchange](https://ai.stackexchange.com/questions/42777/what-is-the-potential-issue-of-nested-neural-networks#:~:text=Isn%27t%20this%20just%20a%20very,are%20vanishing%20and%20exploding%20gradients)). In practice, architectures designed for long-range dependencies (like Transformers) achieve their success by mechanisms that avoid long chains of gradients: e.g., the Transformer’s self-attention directly connects distant tokens, and LSTMs explicitly control gradient flow with gates. A purely nested MLP might find it difficult to match their performance on such tasks. That said, if the task has a known hierarchical structure (like a mathematical expression parsed into a binary tree), recursive neural networks (in the _tree-RNN_ sense) have been effectively used (Socher _et al._ 2011, for parsing and sentiment tasks). Overall, **fractal/nested architectures excel when the problem can be broken into subproblems** solved by sub-networks. They have shown strong results on synthetic tasks requiring hierarchical computation or complex decision boundaries. On typical real-world tasks (like image or language benchmarks), they can work well but often require careful training tricks. For example, FractalNet matched ResNet on CIFAR-100 (22.85% error, which was state-of-art) by using its fractal design with appropriate regularization ([[1605.07648] FractalNet: Ultra-Deep Neural Networks without Residuals](https://arxiv.org/abs/1605.07648#:~:text=is%20transformed%20by%20a%20filter,performance)) – indicating these methods _can_ perform competitively on vision tasks. And the NestNet study found that injecting sub-network activations into a CNN (tested on Fashion-MNIST) improved accuracy compared to a standard CNN of similar size ([](https://proceedings.neurips.cc/paper_files/paper/2022/file/257be12f31dfa7cc158dda99822c6fd1-Paper-Conference.pdf#:~:text=is%20a%2028%20%C3%97%2028,add%20the%20layers%20of%20dropout)) ([](https://proceedings.neurips.cc/paper_files/paper/2022/file/257be12f31dfa7cc158dda99822c6fd1-Paper-Conference.pdf#:~:text=Input%20Conv%20ReLU%20BatchNorm%20Conv,represent%20convolutional%20and%20fully%20connected)). However, the improvements on more complex, large-scale tasks (ImageNet, etc.) have not vastly surpassed mainstream architectures, and the training complexity is higher (NestNets required ~1.5× more training time for those small gains) ([](https://proceedings.neurips.cc/paper_files/paper/2022/file/257be12f31dfa7cc158dda99822c6fd1-Paper-Conference.pdf#:~:text=Table%202%3A%20Test%20accuracy%20comparison,999984)).

**Key Takeaway:** Similar recursive/fractal architectures are **highly expressive** and can solve complex synthetic tasks (like parity or spiral patterns) with ease, often outperforming standard networks with equal top-level capacity ([](https://proceedings.neurips.cc/paper_files/paper/2022/file/257be12f31dfa7cc158dda99822c6fd1-Paper-Conference.pdf#:~:text=Table%202%3A%20Test%20accuracy%20comparison,999984)). They can match top-tier models on some benchmarks (FractalNet vs ResNet) given the right training strategy ([[1605.07648] FractalNet: Ultra-Deep Neural Networks without Residuals](https://arxiv.org/abs/1605.07648#:~:text=is%20transformed%20by%20a%20filter,performance)). But they also tend to be harder to train and not obviously superior on all tasks. In domains like long-sequence modeling, they face the same fundamental challenges as any deep network. This sets the stage for understanding why your Experiment 58 might be plateauing and how to improve it.

## 3. Diagnosing the Accuracy Plateau (~58–60%)

It’s concerning that most runs plateau around 58–60% accuracy (with a best of ~64.5%), even after trying various learning rates, epochs, network sizes, and sub-network depths. This plateau suggests that the network is failing to fully fit the data or escape a poor local optimum. Several likely reasons (supported by known issues in deep and recursive nets) could explain this behavior:

- **Vanishing/Exploding Gradients:** As hinted earlier, a nested neuron structure increases the effective depth of the network dramatically. Even if each sub-network is just 2 layers, a neuron in layer1 feeding into layer2 which feeds into layer3, etc., creates a very deep computation graph. Standard backpropagation may suffer from vanishing gradients through these layers, especially if activation functions like ReLU saturate (e.g., dying ReLUs) or if weights are not initialized well. As one expert succinctly put it, _“once you start nesting them more, the typical issues are vanishing and exploding gradients.”_ ([machine learning - What is the potential issue of nested neural networks - Artificial Intelligence Stack Exchange](https://ai.stackexchange.com/questions/42777/what-is-the-potential-issue-of-nested-neural-networks#:~:text=Isn%27t%20this%20just%20a%20very,are%20vanishing%20and%20exploding%20gradients)) If gradients vanish, early layers and inner neurons get almost no learning signal, causing training to stall at a suboptimal accuracy. Exploding gradients (less likely here, as networks were only grown if stable) could make training unstable, but you would notice erratic loss behavior if that were the case. It’s more likely a slow drift (vanishing gradients leading to tiny weight updates in crucial early layers).

- **Inadequate Training per Mutation:** The evolutionary loop in Experiment 58 trains each mutated network for only a few epochs before evaluation. If most networks are only partially trained, they might not reach anywhere near their potential accuracy. Complex architectures often need longer training to realize their advantages ([](https://proceedings.neurips.cc/paper_files/paper/2022/file/257be12f31dfa7cc158dda99822c6fd1-Paper-Conference.pdf#:~:text=Table%202%3A%20Test%20accuracy%20comparison,999984)). For instance, the NestNet experiments ran for 500 epochs on the spiral task to fully converge ([](https://proceedings.neurips.cc/paper_files/paper/2022/file/257be12f31dfa7cc158dda99822c6fd1-Paper-Conference.pdf#:~:text=0%20100%20200%20300%20400,0%20accuracy%20Standard%20NestNet)). If your loop stops training early (to quickly evaluate many mutations), the population may converge to architectures that _learn fast in a few epochs_ rather than those that _learn best eventually_. This could bias towards simpler solutions that plateau at moderate accuracy. The highest accuracy run (~64.5%) might have stumbled on a structure that learns a bit more rapidly or had lucky initial weights. **Insufficient training time per network can thus create an artificial plateau** – no architecture gets the chance to show its true performance if training is cut short.

- **Local Minima or Structure Trap:** Evolutionary structural search can sometimes get stuck exploring a subspace of architectures that are all similarly limited. If the mutation strategy (growing neurons recursively) tends to produce incremental changes, you might be walking around a plateau in the architecture space. For example, perhaps the network frequently adds subnets in areas that don’t address the dataset’s hardest features, leading to diminishing returns. If most networks share a similar bottleneck (say, an early layer that is too small or a subnetwork that always overfits one part of the data), many variants will plateau similarly. In other words, the algorithm might **explore many structures but not fundamentally alter the network’s ability to represent the needed function** beyond a point. This is analogous to converging to a local optimum in neural architecture search.

- **Overfitting vs. Underfitting Balance:** Interestingly, a ~60% accuracy plateau suggests severe underfitting if this is a standard dataset (e.g., if it’s CIFAR-10, 60% is far below state-of-art ~95%). But it could also be mild overfitting balanced with underfitting (e.g. training accuracy higher but test stuck ~60%). If the dataset is small or the architecture has huge parameter counts (which is possible if neurons recursively expand), the network might overfit substructures and not generalize, yielding a mid-range accuracy. The evolutionary selection might then favor not making it more complex (to avoid worse generalization). Without more detail on dataset and train/test split, both are possible. However, given experimentation with network size didn’t fix it, it hints that just adding capacity (parameters) wasn’t the issue — rather _how_ the network learns is at fault.

- **Optimization Difficulty:** Gradient descent may be struggling with this architecture. The loss surface for a network-within-network is highly non-linear and might have complex interactions between subnetwork weights and top-level weights. Standard optimizers (SGD/Adam) might need very carefully tuned hyperparameters. If the learning rate is off, one part of the network might learn while another barely changes, leading to a partial solution that never coalesces into a higher accuracy. You’ve tried multiple learning rates and epochs, which should have mitigated this somewhat, but **the combination of deep nesting and evolving topology is quite unorthodox for gradient-based optimization**. It’s possible that training gets stuck in a plateau because the gradients needed to escape (to form new features) are extremely small or noisy. As an analogy, this is reminiscent of training very deep plain networks (which was nearly impossible until techniques like batch normalization and residual connections were introduced). Your network is effectively very deep in a nested way, and might be hitting the same wall that early deep networks faced before those innovations.

- **Lack of Regularization or Normalization:** If each “Dimension” subnetwork is being added without techniques like Batch Normalization or Dropout, the model could be internally covariate-shifting a lot – i.e., as one subnetwork’s weights change, the distribution of inputs to higher layers shifts, making training non-stationary. This can slow or stall learning. In the NestNet CNN experiment on Fashion-MNIST, they explicitly added BatchNorm layers and Dropout to _both_ the standard and nested models to keep training stable ([](https://proceedings.neurips.cc/paper_files/paper/2022/file/257be12f31dfa7cc158dda99822c6fd1-Paper-Conference.pdf#:~:text=is%20a%2028%20%C3%97%2028,add%20the%20layers%20of%20dropout)). Without such measures, a nested net might plateau due to internal distribution shifts or overfitting in sub-parts. Given you tried various learning rates and such, it might be worth checking if normalization was part of the design; if not, the plateau could partially be the optimizer fighting an uphill battle against internal chaos.

In summary, the plateau likely arises from **optimization challenges** rather than the fundamental capacity of the architecture. The architecture is expressive (the fact it reached ~64% at least once suggests it can learn), but something is preventing further progress. The usual suspects are vanishing gradients in deep recursive structures, insufficient training time per model, and possibly suboptimal training strategies (lack of normalization, regularization, or proper weight initialization). Next, we’ll suggest remedies to overcome these issues.

## 4. Strategies to Improve Performance

To significantly boost performance, we should improve both the _architecture itself_ and the _training methodology_. Below are several actionable recommendations, grounded in techniques known to help deep or modular networks:

- **Incorporate Skip Connections (Residual Links):** Adding **residual connections** could combat vanishing gradients and ease the optimization of the nested network. In ResNets, skip connections (identity mappings) enabled training of networks 100+ layers deep by providing direct gradient flow to early layers. Even FractalNet, which had no explicit skips, relied on an implicit ensemble of shallow and deep paths to achieve a similar effect ([[1605.07648] FractalNet: Ultra-Deep Neural Networks without Residuals](https://arxiv.org/abs/1605.07648#:~:text=is%20transformed%20by%20a%20filter,performance)). For your architecture, one idea is to allow a neuron’s output to be a _sum_ of the sub-network output and a linear transformation of the original inputs (or the neuron’s direct input). This essentially makes the sub-network a residual module refining the neuron's input. It could be as simple as: `output = subnetwork(x) + W*x` (and possibly non-linear activation after). This way, if the sub-network is hard to train initially, the neuron can fallback to a linear passthrough, and the sub-network learns a residual correction. Residual and highway connections are proven to **improve gradient flow and enable training of very deep architectures** ([[1605.07648] FractalNet: Ultra-Deep Neural Networks without Residuals](https://arxiv.org/abs/1605.07648#:~:text=is%20transformed%20by%20a%20filter,performance)), so integrating them in a recursive structure should help it train deeper “dimensions” without stalling.

- **Use Batch Normalization and/or Layer Normalization:** **Normalization** layers can greatly stabilize and speed up training in deep networks. Batch Normalization (Ioffe & Szegedy, 2015) normalizes activations in each layer, mitigating internal covariate shift and allowing higher learning rates ([Batch Normalization: Accelerating Deep Network Training by ... - arXiv](https://arxiv.org/abs/1502.03167#:~:text=arXiv%20arxiv,regularizer%2C%20in%20some%20cases)). It has been shown to act as a form of regularization and reduce the sensitivity to weight initialization. In the NestNet implementation, the authors added batch norm after each sub-network activation ([](https://proceedings.neurips.cc/paper_files/paper/2022/file/257be12f31dfa7cc158dda99822c6fd1-Paper-Conference.pdf#:~:text=is%20a%2028%20%C3%97%2028,add%20the%20layers%20of%20dropout)), which likely helped the sub-networks integrate smoothly into the larger network. If you haven’t already, consider adding BatchNorm after each neuron’s subnetwork output (and possibly within the subnetwork layers as well). Another option is **Layer Normalization** (especially if batch sizes are small or if using RNN-like structure) to normalize across the neuron's input features. Normalization will help ensure that as you grow new neurons with subnetworks, their outputs don’t saturate or blow up the next layer, and gradients remain in a workable range. In short, it **improves gradient flow and convergence speed** ([Batch Normalization: Accelerating Deep Network Training by ... - arXiv](https://arxiv.org/abs/1502.03167#:~:text=arXiv%20arxiv,regularizer%2C%20in%20some%20cases)), likely raising the attainable accuracy.

- **Improve Weight Initialization:** Proper initialization is crucial for deep or recursive networks. You should ensure that all newly added sub-network weights are initialized in a way that preserves scale. Techniques like Glorot/Xavier initialization (to keep variance constant across layers) or He initialization for ReLUs help avoid early saturation. In a nested network, you might even initialize subnetwork weights to near-_identity_ mappings. For example, set the subnetwork’s final layer weights initially close to zero, so that the neuron’s output is initially small (or initialize the subnetwork to perform an identity operation if possible). This way, when a new subnetwork is added, it doesn’t drastically alter the already learned function – the training can gently fine-tune it from essentially doing nothing to doing something useful. Such an approach is used in **Net2Net** (Chen _et al._, 2016) for expanding networks without disturbing function, and also resonates with how ResNets can be seen as many layers initially just identity mappings that learn residuals. By **initializing new “growth” neutrally**, you reduce the chance of a bad mutation that resets progress.

- **Train Longer or Use Learning Rate Schedules:** If networks plateau at a few epochs, simply training for more epochs (or using a smarter schedule) can help climb out of the plateaus. You might implement an adaptive learning rate schedule – e.g., reduce the learning rate on plateau or use cyclical learning rates – to allow the network to eventually converge. The NestNet experiment ran 500 epochs; your evolution loop might be running say 5 or 10 epochs per network. Perhaps increase that to a larger number once the architecture reaches a certain complexity. Another idea is a two-phase training: during evolution, train briefly to evaluate **fitness**, but keep track of the best few architectures and then **fully train them for many more epochs** to see their true potential. This way, you don’t waste time fully training every mutant, but you do give the promising ones a proper chance. Evolutionary strategies often use such approaches (like **limited evaluation** followed by **refinement of champions**). Also consider using more advanced optimizers (if not already) like **Adam or RAdam** (Rectified Adam). In fact, Shen _et al._ used RAdam to train NestNet ([](https://proceedings.neurips.cc/paper_files/paper/2022/file/257be12f31dfa7cc158dda99822c6fd1-Paper-Conference.pdf#:~:text=T%201%20%E2%8B%85%20,world%20applications%20if%20proper)) ([](https://proceedings.neurips.cc/paper_files/paper/2022/file/257be12f31dfa7cc158dda99822c6fd1-Paper-Conference.pdf#:~:text=problems%3A%20a%20synthetic%20classification%20problem,continuously%20extended%20to%20R%20d)), as it can be more stable for unusual architectures. Ensuring each network is sufficiently trained (or at least using an optimizer that quickly finds a good basin) will raise the accuracy ceiling.

- **Apply Dropout or Drop-Path for Regularization:** Overfitting can sometimes cause a plateau on validation accuracy. Techniques like dropout could prevent subnetworks from co-adapting too tightly. More intriguingly, consider **Drop-Path** (as used in FractalNet ([[1605.07648] FractalNet: Ultra-Deep Neural Networks without Residuals](https://arxiv.org/abs/1605.07648#:~:text=deep,provide%20a%20more%20accurate%20answer)) and later in EfficientNets, etc.). In your context, drop-path would mean randomly disabling some neuron’s subnetwork (either replace it with a linear neuron or zero output) during training. This forces the network to not become overly reliant on a particular deep subnetwork and encourages multiple alternative paths. For example, if you have two layers of neurons each with subnetworks, dropping some subnetworks will force the network to route information through alternate (maybe shallower) routes, essentially training the ensemble of all subpaths. This can improve robustness and might yield a higher eventual accuracy when all parts are active at test time (FractalNet saw that it allowed extraction of high-performance subnets and an “anytime” property of progressively better outputs with more depth ([[1605.07648] FractalNet: Ultra-Deep Neural Networks without Residuals](https://arxiv.org/abs/1605.07648#:~:text=the%20success%20of%20extremely%20deep,provide%20a%20more%20accurate%20answer))). Careful tuning is needed, but this could effectively **unstick** the training from a bad plateau by injecting noise and promoting exploration in function space.

- **Revisit the Evolution Strategy:** If the architecture search itself is suspect, you could refine it. Perhaps introduce smarter mutations (not just random growth). For example, use **Neuro-evolution techniques** like NEAT’s idea of adding nodes with preserved functionality, or **regularized evolution** (as used in Google’s NAS, where aging of models prevents getting stuck in a rut). Another approach is hybrid: use evolution to find a rough architecture, then fine-tune architecture with gradient-based methods (like differentiable architecture search) or just manual adjustments. Also, ensure your fitness metric truly reflects progress – if using validation accuracy after few epochs, maybe augment it with a complexity penalty or an estimate of eventual potential. The goal is to avoid discarding architectures that learn slower but would surpass the plateau given time. **Diversity maintenance** in the population can help (so you don’t converge all individuals to the same mediocre structure).

- **Try Mixture-of-Experts or Attention Components:** To push performance further, you might incorporate ideas from state-of-the-art architectures. For instance, instead of every neuron having a deterministic subnetwork, you could have a small set of subnetwork “experts” and a gating mechanism that decides which expert to use for a given input (this would convert some of the dense capacity into a Mixture-of-Experts layer ([[1701.06538] Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer](https://arxiv.org/abs/1701.06538#:~:text=improvements%20in%20model%20capacity%20with,On%20large%20language%20modeling%20and))). This could improve efficiency and specialization, possibly yielding higher accuracy if different input patterns require different computations. Similarly, adding an **attention mechanism** at some layer could help the network focus its sub-networks on the most relevant parts of the input, which might be especially useful in tasks with many features or sequence elements. These additions move your architecture closer to modern designs (Transformers use attention to great success, and MoEs have shown extreme scalability), while still retaining the recursive subnetwork flavor.

- **Task-Specific Adjustments:** Depending on the domain of your classification task, you may need to tailor the architecture. If it’s an image task, consider adding some convolutional front-end or at least weight sharing in early layers, because a fully-connected nested network might be data-inefficient on images. If it’s a sequence, consider unrolling the network recursively over sequence positions (making it closer to a recurrent net) or just switching to a Transformer baseline for comparison. Sometimes these recursive models shine more on structured tasks (like parse trees, as in NLP) rather than generic classification. So ensure the architecture is playing to its strengths – for instance, if the task has hierarchical structure, try to align the network’s recursive growth with that (like if classifying documents, a neuron’s subnetwork could explicitly process a sentence, etc.).

Implementing the above improvements can be non-trivial, but each addresses a likely issue: **skip connections and normalization tackle vanishing gradients; better initialization and training schedules tackle optimization difficulties; dropout/drop-path and MoE concepts tackle overfitting and specialization; and adjusting the evolution/search can prevent getting stuck**. Adopting even a few of these could potentially break the 60% ceiling – for example, simply adding batch norm and residual connections might allow you to train much deeper sub-networks without plateauing, which in turn yields higher accuracy.

## 5. Recommendations on Future Direction

Considering all the above, we must weigh the **research value vs. practical payoff** of this recursive neuron-with-subnetwork design:

- **Potential Upside:** The architecture is undoubtedly novel and theoretically compelling. It significantly expands the representational power of a neural network for a given width/depth. The NestNet theory proves that adding this recursive “height” dimension can exponentially improve function approximation efficiency ([](https://proceedings.neurips.cc/paper_files/paper/2022/file/257be12f31dfa7cc158dda99822c6fd1-Paper-Conference.pdf#:~:text=net%02work%20%28NestNet%29,while%20the%20optimal%20approximation)). If your ultimate goal is to explore new frontiers in model architecture (e.g., writing a research paper or creating a new kind of neural network library), it could be worth pursuing further. You might be uncovering important insights about training deep nested structures. With the improvements suggested (residual connections, better training regimes, etc.), there’s a good chance you could push accuracy much higher, perhaps to compete with conventional networks on the same task. Successfully training such networks on complex tasks would be a valuable contribution, as current literature (NestNet, FractalNet) has only begun to scratch the surface, often with constraints (e.g., NestNet used very small subnetworks for tractability ([](https://proceedings.neurips.cc/paper_files/paper/2022/file/257be12f31dfa7cc158dda99822c6fd1-Paper-Conference.pdf#:~:text=2%20and%20we%20introduce%20a,w1%2C%20b0%20%E2%88%88%20R%203))). There may also be specific problem domains where this approach outperforms others – for example, cases where interpretability of hierarchical features is important, or when you have a known recursive structure in the data (such as recursive schemas or algorithms that you want a network to learn).

- **Practical Considerations:** On the other hand, if your immediate aim is high accuracy on standard benchmarks or building a product, **this approach is high-risk and may yield slower progress** than using known architectures. Transformers, for instance, have become a strong baseline across many tasks (vision, language, audio) by virtue of their ability to capture long-range interactions and scale up with data. A Transformer or a well-tuned ResNet might achieve 90%+ accuracy on a task where your experiment is stuck at 60%, simply because those architectures have been optimized and battle-tested by the community. Similarly, Mixture-of-Experts models (and related techniques like ensemble learning or multi-branch networks) provide ways to increase capacity and flexibility with much less training difficulty — e.g., an MoE essentially trains many smaller networks and learns to route inputs between them, which sidesteps some of the deep optimization issues and has been shown to excel in very large-scale settings ([[1701.06538] Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer](https://arxiv.org/abs/1701.06538#:~:text=Experts%20layer%20,art%20at%20lower%20computational%20cost)). From a **time and resource perspective**, pivoting to a known strong baseline could drastically increase performance in the short term. It would also give you a reference point: for example, train a standard deep network (say, a 3-layer MLP with similar parameter count, or a Random Forest, or XGBoost if tabular data) on your task to see what accuracy is achievable. If that baseline already gets, say, 80%+, then the fact your complex model is at 60% suggests a major training issue. You might then use the baseline as a sanity check while you incrementally fix the recursive model.

- **Hybrid Approach:** These two paths aren’t mutually exclusive. You might use a baseline like a Transformer or ResNet as a **control** to understand your task’s difficulty. If the baseline far exceeds your model, focus on narrowing that gap with the improvements above. If you manage to get the recursive model competitive, that’s a strong endorsement to continue. If not, you have to decide how much the novel architecture is worth to you versus simply using what works. In some cases, researchers conclude that a novel idea, while interesting, doesn’t outperform simpler methods, and thus they pivot their research. In others, they identify a niche where the idea shines.

- **Complexity and Compute Cost:** Another factor is that your recursive architecture, especially grown via evolution, can become **computationally expensive** (many layers inside neurons, etc.). If training is taking too long or models are too large, this is a practical hindrance. Transformers and MoEs, while also large, have the benefit of massive industry investment and optimized libraries. Your custom approach might not leverage GPUs efficiently if it’s unusual (depending on your implementation). Sometimes simplifying the architecture yields disproportionately higher gains in speed and only minor losses in expressiveness. For instance, if each neuron’s subnetwork is shallow (1 hidden layer) instead of very deep, you may still capture some of the benefits without as much difficulty. Or you could restrict recursion depth and increase breadth (more neurons) to simplify. These are fallback options if after many attempts the deep recursion doesn’t pay off.

**Conclusion:** If the purpose of Experiment 58 is research and learning, it’s worth addressing its current limitations and seeing how far it can go. The concept aligns with cutting-edge ideas (nested networks, fractals) that _are_ being actively studied ([](https://proceedings.neurips.cc/paper_files/paper/2022/file/257be12f31dfa7cc158dda99822c6fd1-Paper-Conference.pdf#:~:text=height%2C%20in%20addition%20to%20width,parameters%29.%20The)), so continuing could yield publishable insights. However, be prepared to implement advanced training techniques and invest time into debugging/training – essentially, you’d be on the frontier, dealing with issues that few tools or prior papers directly solve. Conversely, if your priority is to solve a problem or win a competition, you might get there faster with a known **strong baseline** (e.g., a Transformer if the data is sequential or has complex interactions, or an ensemble of simpler models). Often in machine learning, **starting simple** is recommended: ensure that a baseline can achieve the desired accuracy, and use that as a benchmark for any complex model. If after applying improvements your recursive model still lags far behind a simpler network, it would be a signal to rethink. It might indicate that the added complexity isn’t yielding proportional benefit, and thus exploring a different direction (or simplifying the model) could be more fruitful.

In summary, **the recursive subnetwork design is promising but challenging**. By integrating known best practices (residuals, normalization, proper training schedules) you can likely overcome the current plateau. This could unlock much higher accuracies, demonstrating the power of the approach. If those efforts still fall short, then for practical purposes, switching to architectures with a proven track record (Transformers, ResNets, MoEs, etc.) is advisable to achieve performance targets. Either outcome will be instructive – you will have either advanced the understanding of this novel architecture or confirmed that a more straightforward model is the better tool for the job. Always keep the end goal in mind and allocate your time/compute accordingly. Good luck with Experiment 58 and its successors!

**References:**

- Lin, M. _et al._ (2014). _Network In Network._ (Introduces micro-MLP “layers” inside conv nets to increase modeling power) ([](https://arxiv.org/pdf/1312.4400#:~:text=nonlinear%20function%20of%20the%20input,Both%20the)).
- Shen, Z. _et al._ (2022). _Neural Networks beyond Depth and Width (NestNet)._ (Defines nested networks with “height” – neurons activated by sub-networks) ([](https://proceedings.neurips.cc/paper_files/paper/2022/file/257be12f31dfa7cc158dda99822c6fd1-Paper-Conference.pdf#:~:text=a%20nested%20structure%2C%20and%20hence,Lipschitz%20continuous)) ([](https://proceedings.neurips.cc/paper_files/paper/2022/file/257be12f31dfa7cc158dda99822c6fd1-Paper-Conference.pdf#:~:text=Table%202%3A%20Test%20accuracy%20comparison,999984)).
- Larsson, G. _et al._ (2017). _FractalNet: Ultra-Deep Networks without Residuals._ (Uses a self-similar fractal architecture; matches ResNet performance on CIFAR/ImageNet) ([[1605.07648] FractalNet: Ultra-Deep Neural Networks without Residuals](https://arxiv.org/abs/1605.07648#:~:text=is%20transformed%20by%20a%20filter,performance)).
- Andreas, J. _et al._ (2016). _Neural Module Networks._ (Composes networks from smaller modules for VQA, exploiting compositional structure) ([[1511.02799] Neural Module Networks](https://arxiv.org/abs/1511.02799#:~:text=compositional%20linguistic%20structure%20of%20questions,a%20new%20dataset%20of%20complex)).
- Shazeer, N. _et al._ (2017). _Outrageously Large Networks: Sparsely-Gated Mixture-of-Experts._ (Demonstrates scaling to 137B-parameter models with many sub-networks, achieving SOTA results) ([[1701.06538] Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer](https://arxiv.org/abs/1701.06538#:~:text=improvements%20in%20model%20capacity%20with,On%20large%20language%20modeling%20and)).
- Rahmat, R. _et al._ (2015). _Gene-Regulated Nested Networks._ (Uses a nested network to solve 8-bit parity XOR problem efficiently) ([A Gene-Regulated Nested Neural Network](https://ccis2k.org/iajit/index.php/archive/volume-12-2015/november-2015-no-6/item/1429-a-gene-regulated-nested-neural-network#:~:text=machine%20development%20and%20knowledge%20discovery,and%20meets%20the%20required%20objectives)).
- **Additional:** Ioffe, S. & Szegedy, C. (2015). _Batch Normalization._ (Speeds up and stabilizes deep network training) ([Batch Normalization: Accelerating Deep Network Training by ... - arXiv](https://arxiv.org/abs/1502.03167#:~:text=arXiv%20arxiv,regularizer%2C%20in%20some%20cases)); Discussion on AI StackExchange about nested nets and gradient issues ([machine learning - What is the potential issue of nested neural networks - Artificial Intelligence Stack Exchange](https://ai.stackexchange.com/questions/42777/what-is-the-potential-issue-of-nested-neural-networks#:~:text=Isn%27t%20this%20just%20a%20very,are%20vanishing%20and%20exploding%20gradients)).
